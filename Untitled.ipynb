{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5e2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from imutils.video import VideoStream\n",
    "import os\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Define paths for data collection and model storage\n",
    "data_path = \"face_dataset\"  # Replace with your dataset path\n",
    "model_path = \"face_recognition_model.h5\"  # Trained model output path\n",
    "\n",
    "# Function to collect facial data for a new person\n",
    "def collect_facial_data(person_name):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "    # Create a directory for the person's data if it doesn't exist\n",
    "    os.makedirs(os.path.join(data_path, person_name), exist_ok=True)\n",
    "\n",
    "    num_samples = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Liveness detection (optional): Ask user to blink or move head\n",
    "            # Implement logic to check for blinks or head movement within a timeframe\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            num_samples += 1\n",
    "            cv2.imwrite(os.path.join(data_path, person_name, f\"{num_samples}.jpg\"), roi_color)\n",
    "\n",
    "        cv2.putText(frame, f\"Samples Collected: {num_samples}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Collecting Facial Data', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        if key == ord('q') or num_samples >= 100:  # Collect 100 samples or press 'q' to quit\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Collect data for new people\n",
    "while True:\n",
    "    person_name = input(\"Enter person's name (or 'q' to quit): \")\n",
    "    if person_name == 'q':\n",
    "        break\n",
    "    collect_facial_data(person_name)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# Function to train the CNN model\n",
    "def train_cnn_model():\n",
    "   \n",
    "    # Load images and labels from data directory\n",
    "    images = []\n",
    "    labels = []\n",
    "    for person_name in os.listdir(data_path):\n",
    "        for filename in os.listdir(os.path.join(data_path, person_name)):\n",
    "            img_path = os.path.join(data_path, person_name, filename)\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.resize(img, (100, 100))  # Resize images for CNN input\n",
    "            images.append(img)\n",
    "            labels.append(person_name)\n",
    "\n",
    "    # Convert images and labels to NumPy arrays\n",
    "    images = np.array(images, dtype=np.float32) / 255.0\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # One-hot encode labels for categorical crossentropy loss\n",
    "   \n",
    "    label_encoder = LabelEncoder()\n",
    "    labels = label_encoder.fit_transform(labels)\n",
    "    onehot_encoder = OneHotEncoder(sparse=False)\n",
    "    labels = onehot_encoder.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "    # Build and train the CNN model\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', input_shape=(100, 100, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer with num_classes\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Split data into training and validation sets (optional)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model (adjust epochs based on dataset size and computational resources)\n",
    "    model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))  # Consider validation for overfitting\n",
    "\n",
    "    # Save the trained model\n",
    "    model.save(model_path)  # Use the defined model_path\n",
    "\n",
    "# Train the CNN model\n",
    "train_cnn_model()\n",
    "\n",
    "print(\"Model training complete!\")\n",
    "\n",
    "# Define paths and variables\n",
    "data_path = \"face_dataset\"\n",
    "model_path = \"face_recognition_model.h5\"\n",
    "attendance_file = \"attendance.xlsx\"\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Function to check for blinks or head movement within a timeframe\n",
    "def is_live(frame, prev_frame, threshold=10):\n",
    "    # Implement your logic here (e.g., compare difference between frames, track eye regions)\n",
    "    # You can use libraries like OpenCV's SURF features for motion detection\n",
    "    # Return True if liveness is detected, False otherwise\n",
    "    return True  # Replace with your implementation\n",
    "\n",
    "# Function to recognize face and mark attendance\n",
    "def recognize_and_mark_attendance(model, label_encoder):\n",
    "    # Load the trained CNN model\n",
    "    model = load_model(model_path)\n",
    "\n",
    "    # Create an Excel workbook for attendance\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Date\", \"Time\", \"Name\"])  # Header row\n",
    "\n",
    "    # Start video stream\n",
    "    vs = VideoStream(src=0).start()\n",
    "    while True:\n",
    "        frame = vs.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]  # Region of interest for face\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "            # Liveness detection (replace with your implementation)\n",
    "            prev_frame = frame  # Assuming you have a mechanism to store the previous frame\n",
    "            if not is_live(roi_color, prev_frame):\n",
    "                cv2.putText(frame, \"Not Live Face Detected\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                continue\n",
    "\n",
    "            # Resize for model input\n",
    "            img = cv2.resize(roi_gray, (100, 100))\n",
    "            img = np.expand_dims(img, axis=0)  # Add a dimension for batch processing\n",
    "            img = np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "            # Predict probability distribution for each class (person)\n",
    "            predictions = model.predict(img)[0]\n",
    "\n",
    "            # Find the class with the highest probability\n",
    "            max_index = np.argmax(predictions)\n",
    "            predicted_name = label_encoder.inverse_transform([max_index])[0]\n",
    "            proba = predictions[max_index]\n",
    "\n",
    "            # Display name and confidence level if probability is high enough\n",
    "            if proba > 0.8:  # Adjust threshold based on your model's performance\n",
    "                cv2.putText(frame, f\"{predicted_name} ({proba:.2f})\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                # Mark attendance in Excel\n",
    "                now = datetime.datetime.now()\n",
    "                ws.append([now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M:%S\"), predicted_name])\n",
    "\n",
    "            else:\n",
    "                cv2.putText(frame, \"Unknown\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "        cv2.imshow('Attendance System', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Save attendance data and quit on 'q' press\n",
    "        if key == ord('q'):\n",
    "            wb.save(attendance_file)\n",
    "            break\n",
    "\n",
    "    vs.stop()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63339614",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e5c1ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e29e0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from imutils.video import VideoStream\n",
    "from datetime import timedelta\n",
    "from tensorflow.keras.models import load_model\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Define paths for data collection and model storage\n",
    "data_path = \"face_dataset\"  # Replace with your dataset path\n",
    "model_path = \"face_recognition_model.h5\"  # Trained model output path\n",
    "\n",
    "\n",
    "def is_live(frame, prev_frame, threshold=10):\n",
    "    # Blink detection (basic implementation)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Thresholding to isolate foreground (face)\n",
    "    thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh_prev = cv2.threshold(gray_prev, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "\n",
    "    # Calculate difference between current and previous frames\n",
    "    difference = cv2.absdiff(thresh, thresh_prev)\n",
    "\n",
    "    # Count the number of non-zero pixels (indicating change)\n",
    "    num_changed_pixels = cv2.countNonZero(difference)\n",
    "\n",
    "    # Threshold for blink detection (adjust based on image resolution)\n",
    "    blink_threshold = 500\n",
    "\n",
    "    # Check if number of changed pixels exceeds blink threshold\n",
    "    is_blink = num_changed_pixels > blink_threshold\n",
    "\n",
    "    # Head movement detection (basic implementation)\n",
    "    # Calculate absolute difference between frames\n",
    "    diff = cv2.absdiff(frame, prev_frame)\n",
    "    diff_sum = cv2.sumElems(diff)[0] + cv2.sumElems(diff)[1] + cv2.sumElems(diff)[2]\n",
    "\n",
    "    # Threshold for head movement detection (adjust based on image resolution)\n",
    "    movement_threshold = 10000  # Experiment to find a suitable value\n",
    "\n",
    "    # Check if the difference between frames exceeds movement threshold\n",
    "    is_movement = diff_sum > movement_threshold\n",
    "\n",
    "    # Combine blink and head movement for liveness check\n",
    "    is_live = is_blink or is_movement  # You can adjust the logic (e.g., AND)\n",
    "    print(is_live)\n",
    "    print(is_blink)\n",
    "    print(is_movement)\n",
    "    return is_live, is_blink, is_movement  # Return additional information for debugging\n",
    "\n",
    "\n",
    "def collect_facial_data(person_name):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "    # Create a directory for the person's data if it doesn't exist\n",
    "    os.makedirs(os.path.join(data_path, person_name), exist_ok=True)\n",
    "\n",
    "    num_samples = 0\n",
    "    prev_frame = None  # Store the previous frame for comparison\n",
    "    blink_count = 0  # Track number of blinks within a timeframe\n",
    "    start_time = None  # Time when collection starts\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "            \n",
    "            # Liveness detection (blink and head movement)\n",
    "            is_live, is_blink, is_movement = is_live(roi_color, prev_frame)\n",
    "            print(is_live,' ',is_blink,' ',is_movement)\n",
    "            prev_frame = frame  # Update previous frame for next iteration\n",
    "\n",
    "            if is_live:\n",
    "                # Reset blink count and start time if live face detected\n",
    "                blink_count = 0\n",
    "                start_time = datetime.datetime.now()\n",
    "\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                num_samples += 1\n",
    "                cv2.imwrite(os.path.join(data_path, person_name, f\"{num_samples}.jpg\"), roi_color)\n",
    "\n",
    "                # Check for blinks within a timeframe (adjust timeout as needed)\n",
    "                if is_blink:\n",
    "                    blink_count += 1\n",
    "                    timeout = timedelta(seconds=3)  # Adjust timeout for blinks\n",
    "\n",
    "                    if (datetime.datetime.now() - start_time) > timeout and blink_count >= 2:\n",
    "                        print(f\"Collected enough blinks for {person_name}\")\n",
    "                        break  # Collected enough blinks, exit loop\n",
    "\n",
    "            else:\n",
    "                cv2.putText(frame, \"Not Live Face Detected\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        cv2.putText(frame, f\"Samples Collected: {num_samples}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Collecting Facial Data', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit on 'q' press or after collecting enough samples\n",
    "        if key == ord('q') or num_samples >= 50:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ... (other code)\n",
    "\n",
    "collect_facial_data('akash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1787ef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c440542",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabae6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5d47d2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from imutils.video import VideoStream\n",
    "from datetime import timedelta\n",
    "from tensorflow.keras.models import load_model\n",
    "from openpyxl import Workbook\n",
    "\n",
    "# Define paths for data collection and model storage\n",
    "data_path = \"face_dataset\"  # Replace with your dataset path\n",
    "model_path = \"face_recognition_model.h5\"  # Trained model output path\n",
    "\n",
    "def collect_facial_data(person_name):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "    # Create a directory for the person's data if it doesn't exist\n",
    "    os.makedirs(os.path.join(data_path, person_name), exist_ok=True)\n",
    "\n",
    "    num_samples = 0\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y:y+h, x:x+w]\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x+w+20, y+h+20), (0, 255, 0), 2)\n",
    "            num_samples += 1\n",
    "            cv2.imwrite(os.path.join(data_path, person_name, f\"{num_samples}.jpg\"), roi_color)\n",
    "\n",
    "\n",
    "        cv2.putText(frame, f\"Samples Collected: {num_samples}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Collecting Facial Data', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Quit on 'q' press or after collecting enough samples\n",
    "        if key == ord('q') or num_samples >= 50:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# ... (other code)\n",
    "\n",
    "collect_facial_data('akash')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2680fe8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32986b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c500e67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f29d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define paths for data collection and model storage\n",
    "data_path = \"face_dataset\"  # Replace with your dataset path\n",
    "model_path = \"face_recognition_model.h5\"  # Trained model output path\n",
    "\n",
    "names = []        # Initialize an empty list to store names\n",
    "if os.path.exists('face_dataset') :\n",
    "    names.extend(os.listdir('face_dataset'))\n",
    "        \n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "def collect_facial_data(person_name):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "    # Create a directory for the person's data if it doesn't exist\n",
    "    os.makedirs(os.path.join(data_path, person_name), exist_ok=True)\n",
    "    if os.path.exists('face_dataset') :\n",
    "        names.extend(os.listdir('face_dataset'))\n",
    "    \n",
    "    num_samples = 0\n",
    "    prev_frame = None  # Store the previous frame for comparison\n",
    "    blink_count = 0  # Track number of blinks within a timeframe\n",
    "    start_time = None  # Time when collection starts\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame,1)\n",
    "        \n",
    "#         color = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        faces = face_cascade.detectMultiScale(frame, 1.8, 5)\n",
    "    \n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "#             roi_gray = gray[y:y+h, x:x+w]\n",
    "            roi_color = frame[y-68:y+h+18, x-28:x+w+38]\n",
    "            \n",
    "            # Liveness detection (blink movement)\n",
    "            global is_live\n",
    "            \n",
    "            # Check if ROI is empty before proceeding\n",
    "            if roi_color is not None and roi_color.any():\n",
    "                # Liveness detection (blink and head movement)\n",
    "                is_live, is_blink = if_live(roi_color, prev_frame)\n",
    "                print(is_live,' ',is_blink,' ')\n",
    "                prev_frame = frame.copy() # Update previous frame for next iteration\n",
    "            else:\n",
    "                continue  # Skip this iteration if ROI is empty\n",
    "                \n",
    "            if is_live:\n",
    "                # Reset blink count and start time if live face detected\n",
    "                blink_count = 0\n",
    "                start_time = datetime.datetime.now()\n",
    "\n",
    "                cv2.rectangle(frame, (x-30, y-70), (x+w+40, y+h+20), (0, 255, 0), 2)\n",
    "                num_samples += 1\n",
    "                cv2.imwrite(os.path.join(data_path, person_name, f\"{num_samples}.jpg\"), roi_color)\n",
    "               \n",
    "            # Check for blinks within a timeframe (adjust timeout as needed)\n",
    "                if is_blink:\n",
    "                    blink_count += 1\n",
    "                    timeout = timedelta(seconds=3)  # Adjust timeout for blinks\n",
    "\n",
    "                    if (datetime.datetime.now() - start_time) > timeout and blink_count >= 2:\n",
    "                        print(f\"Collected enough blinks for {person_name}\")\n",
    "                        break  # Collected enough blinks, exit loop\n",
    "            else:\n",
    "                cv2.putText(frame, \"Not Live Face Detected\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        \n",
    "        cv2.putText(frame, f\"Samples Collected: {num_samples}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Collecting Facial Data', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Quit on 'q' press or after collecting enough samples\n",
    "        if key == ord('q') or num_samples >= 100:\n",
    "            break\n",
    "            \n",
    "    # Add the new person's name to the list\n",
    "    names.append(person_name)\n",
    "\n",
    "    # Update the label encoder with the expanded list of names\n",
    "    \n",
    "    label_encoder.fit(names)\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\n",
    "# ... (other code)\n",
    "label_encoder.fit(names)\n",
    "\n",
    "# collect_facial_data('Rohit')\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
