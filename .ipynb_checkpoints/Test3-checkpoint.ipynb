{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5013584",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import datetime\n",
    "from imutils.video import VideoStream\n",
    "from datetime import timedelta\n",
    "from tensorflow.keras.models import load_model\n",
    "from openpyxl import Workbook\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9da3abc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from joblib import dump , load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "675cc214",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_live = False\n",
    "\n",
    "\n",
    "def if_live(frame, prev_frame, threshold=10):\n",
    "    global is_live  # Access the global variable\n",
    "    \n",
    "    if frame is None or prev_frame is None :\n",
    "        return False,False\n",
    "    \n",
    "    # Blink detection (basic implementation)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    gray_prev = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Thresholding to isolate foreground (face)\n",
    "    thresh = cv2.threshold(gray, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "    thresh_prev = cv2.threshold(gray_prev, 127, 255, cv2.THRESH_BINARY)[1]\n",
    "    \n",
    "    thresh_prev_resized = cv2.resize(thresh_prev, (thresh.shape[1], thresh.shape[0]))\n",
    "\n",
    "    # Calculate difference between current and previous frames\n",
    "    difference = cv2.absdiff(thresh, thresh_prev_resized)\n",
    "\n",
    "    # Count the number of non-zero pixels (indicating change)\n",
    "    num_changed_pixels = cv2.countNonZero(difference)\n",
    "\n",
    "    # Threshold for blink detection (adjust based on image resolution)\n",
    "    blink_threshold = 500\n",
    "\n",
    "    # Check if number of changed pixels exceeds blink threshold\n",
    "    is_blink = num_changed_pixels > blink_threshold\n",
    "    is_live = is_blink  \n",
    "\n",
    "    return is_live # Return additional information for debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6948b608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LabelEncoder()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LabelEncoder</label><div class=\"sk-toggleable__content\"><pre>LabelEncoder()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Define paths for data collection and model storage\n",
    "data_path = \"face_dataset\"  # Replace with your dataset path\n",
    "model_path = \"face_recognition_model.h5\"  # Trained model output path\n",
    "\n",
    "names = []        # Initialize an empty list to store names\n",
    "if os.path.exists('face_dataset') :\n",
    "    names.extend(os.listdir('face_dataset'))\n",
    "        \n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "\n",
    "def collect_facial_data(person_name):\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "    cap = cv2.VideoCapture(0)  # Use 0 for default webcam\n",
    "\n",
    "    # Create a directory for the person's data if it doesn't exist\n",
    "    os.makedirs(os.path.join(data_path, person_name), exist_ok=True)\n",
    "    if os.path.exists('face_dataset') :\n",
    "        names.extend(os.listdir('face_dataset'))\n",
    "    \n",
    "    num_samples = 0\n",
    "\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        frame = cv2.flip(frame,1)\n",
    "        faces = face_cascade.detectMultiScale(frame, 1.8, 5)\n",
    "    \n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "          \n",
    "            roi_color = frame[y-68:y+h+18, x-28:x+w+38]\n",
    "\n",
    "            cv2.rectangle(frame, (x-30, y-70), (x+w+40, y+h+20), (0, 255, 0), 2)\n",
    "            num_samples += 1\n",
    "            cv2.imwrite(os.path.join(data_path, person_name, f\"{num_samples}.jpg\"), roi_color)\n",
    "        \n",
    "        cv2.putText(frame, f\"Samples Collected: {num_samples}\", (10, 30),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "\n",
    "        cv2.imshow('Collecting Facial Data', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        \n",
    "        # Quit on 'q' press or after collecting enough samples\n",
    "        if key == ord('q') or num_samples >= 100:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            break\n",
    "            \n",
    "    # Add the new person's name to the list\n",
    "    names.append(person_name)\n",
    "\n",
    "    # Update the label encoder with the expanded list of names \n",
    "    label_encoder.fit(names)\n",
    "    \n",
    "\n",
    "# ... (other code)\n",
    "label_encoder.fit(names)\n",
    "\n",
    "# collect_facial_data('test_name')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f20bde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load images and labels from data directory\n",
    "images = []\n",
    "labels = []\n",
    "for person_name in os.listdir(data_path):\n",
    "    for filename in os.listdir(os.path.join(data_path, person_name)):\n",
    "        img_path = os.path.join(data_path, person_name, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (200, 200))  # Resize images for CNN input\n",
    "        images.append(img)\n",
    "        labels.append(person_name)\n",
    "\n",
    "# Convert images and labels to NumPy arrays\n",
    "images = np.array(images, dtype=np.float32) / 255.0\n",
    "labels = np.array(labels)\n",
    "\n",
    "\n",
    "# One-hot encode labels for categorical crossentropy loss   \n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "labels = onehot_encoder.fit_transform(labels.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df8b496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train the CNN model\n",
    "def train_cnn_model():\n",
    "   \n",
    "    # Data Augmentation\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=20,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest'\n",
    "    )\n",
    "\n",
    "\n",
    "    # Build and train the CNN model\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First convolutional layer     \n",
    "    model.add(Conv2D(32, kernel_size=(3, 3), padding='same',activation='relu', kernel_initializer='he_uniform', input_shape=(200, 200, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    #Second Convolultional layer \n",
    "    model.add(Conv2D(64, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    \n",
    "    #Third convolutional layer     \n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), padding='same', activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Dropout for regularization\n",
    "    model.add(Dropout(0.25))\n",
    "    \n",
    "    # Flatten the output of the convolutional layers\n",
    "    model.add(Flatten())\n",
    "    \n",
    "    # Dense layers for classification\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(label_encoder.classes_), activation='softmax'))  # Output layer with num_classes\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Split data into training and validation sets (optional)\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Train the model (adjust epochs based on dataset size and computational resources)\n",
    "    history=model.fit(X_train, y_train, epochs=20, batch_size=32, validation_data=(X_val, y_val))  # Consider validation for overfitting\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training and Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    model.summary()\n",
    "    # Save the trained model\n",
    "    model.save(model_path)  # Use the defined model_path\n",
    "      \n",
    "\n",
    "# Train the CNN model\n",
    "train_cnn_model()\n",
    "\n",
    "print(\"Model training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d686966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  confusion matrix\n",
    "\n",
    "    \n",
    "# Split data into training and validation sets (optional)    \n",
    "X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Load the trained CNN model\n",
    "model = load_model(model_path)\n",
    "# Predict labels for validation set\n",
    "y_pred = model.predict(X_val)\n",
    "# Convert one-hot encoded labels to categorical labels\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_val_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "# Compute confusion matrix\n",
    "confusion_mat = confusion_matrix(y_val_classes, y_pred_classes)\n",
    "print(\"Confusion Matrix:\")\n",
    "# print(confusion_mat)\n",
    "   \n",
    "    \n",
    "# Visualize confusion matrix using box plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(confusion_mat, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93fb914",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse classification report string and extract metrics\n",
    "def parse_classification_report(report):\n",
    "    lines = report.split('\\n')\n",
    "    data = [line.split() for line in lines[2:-5]]\n",
    "    classes = [line.split()[0] for line in lines[2:-5]]\n",
    "    metrics = ['precision', 'recall', 'f1-score', 'support']\n",
    "    df_data = []\n",
    "    for d in data:\n",
    "        row_data = [float(x) for x in d[1:]]\n",
    "        df_data.append(row_data)\n",
    "    df = pd.DataFrame(df_data, columns=metrics, index=classes)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "# Compute and print classification report\n",
    "class_report = classification_report(y_val_classes, y_pred_classes, target_names=label_encoder.classes_)\n",
    "print(\"Classification Report:\")\n",
    "print(class_report)\n",
    "\n",
    "# Parse classification report and convert it to DataFrame\n",
    "class_report_df = parse_classification_report(class_report)\n",
    "\n",
    "# Plot metrics for each class\n",
    "for metric in class_report_df.columns:\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    sns.barplot(x=class_report_df.index, y=class_report_df[metric])\n",
    "    plt.title(f'{metric.capitalize()} per Class')\n",
    "    plt.xlabel('Class')\n",
    "    plt.ylabel(metric.capitalize())\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24414243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths and variables\n",
    "data_path = \"face_dataset\"\n",
    "model_path = \"face_recognition_model.h5\"\n",
    "attendance_file = \"attendance.xlsx\"\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "recorded_names = set() # Create an empty set to store already recorded names\n",
    "\n",
    "# Function to recognize face and mark attendance\n",
    "def recognize_and_mark_attendance(model):\n",
    "    # Load the trained CNN model\n",
    "    model = load_model(model_path)\n",
    "    svm_model = load('svm_model.joblib')\n",
    "    global label_encoder  # Access the global label encoder\n",
    "    # Load the label encoder if not already loaded\n",
    "       \n",
    "    # Create an Excel workbook for attendance\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Date\", \"Time\", \"Name\"])  # Header row\n",
    "\n",
    "    # Start video stream\n",
    "    vs = VideoStream(src=0).start()\n",
    "    while True:\n",
    "        frame = vs.read()\n",
    "#         gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        faces = face_cascade.detectMultiScale(frame, 1.8, 5)\n",
    "        num_samples = 0\n",
    "        prev_frame = None  # Store the previous frame for comparison\n",
    "        start_time = None  # Time when collection starts\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_color = frame[y-68:y+h+18, x-28:x+w+38]\n",
    "            \n",
    "            # Liveness detection (replace with your implementation)\n",
    "            prev_frame = frame  # Assuming you have a mechanism to store the previous frame\n",
    "            \n",
    "            # Liveness detection (blink movement)\n",
    "            global is_live\n",
    "            \n",
    "            \n",
    "            # Check if ROI is empty before proceeding\n",
    "            if roi_color is not None and roi_color.any():\n",
    "                # Liveness detection (blink and head movement)\n",
    "                is_live = if_live(roi_color, prev_frame)\n",
    "                prev_frame = frame.copy() # Update previous frame for next iteration\n",
    "            else:\n",
    "                continue  # Skip this iteration if ROI is empty\n",
    "                \n",
    "            if is_live:\n",
    "                # Reset blink count and start time if live face detected\n",
    "#                 cv2.rectangle(frame, (x-30, y-70), (x+w+40, y+h+20), (0, 255, 0), 2)\n",
    "                # Resize for model input\n",
    "                img = cv2.resize(roi_color, (200, 200))\n",
    "                img = np.expand_dims(img, axis=0)  # Add a dimension for batch processing\n",
    "                img = np.array(img, dtype=np.float32) / 255.0\n",
    "\n",
    "                # Predict probability distribution for each class (person)\n",
    "                predictions = model.predict(img)[0]\n",
    "                svm_predicitons=svm_model.predict(img)\n",
    "\n",
    "                # Find the class with the highest probability\n",
    "                max_index = np.argmax(predictions)\n",
    "                svm_max_index=np.argmax(svm_predictions)\n",
    "                predicted_name = label_encoder.inverse_transform([max_index])[0]\n",
    "                svm_predicted_name=label_encoder.inverse_transform([svm_max_index])[0]\n",
    "                proba = predictions[max_index]\n",
    "                svm_proba = svm_predicitions[svm_max_index]\n",
    "                print('svm predicted name ',sum_predicted_name)\n",
    "\n",
    "                # Display name and confidence level if probability is high enough\n",
    "                if predicted_name in recorded_names and proba > 0.8:  # Check if already recorded\n",
    "                    print(f\"{predicted_name} already marked attendance.\") \n",
    "                    cv2.putText(frame, f\"{predicted_name} ({proba:.2f})\", (x-30, y - 75),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                elif proba > 0.6:  # Adjust threshold based on your model's performance\n",
    "                    cv2.putText(frame, f\"{predicted_name} ({proba:.2f})\", (x-30, y - 75),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                    recorded_names.add(predicted_name)\n",
    "                    # Mark attendance in Excel\n",
    "                    now = datetime.datetime.now()\n",
    "                    ws.append([now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M:%S\"), predicted_name])\n",
    "\n",
    "                else:\n",
    "                    cv2.putText(frame, \"Unknown\", (x-30, y - 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "                cv2.rectangle(frame, (x-30, y-70), (x+w+20, y+h+40), (255, 0, 0), 2)\n",
    "            \n",
    "            \n",
    "            \n",
    "            else :\n",
    "                cv2.putText(frame, \"Not Live Face Detected\", (x, y - 10),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                continue\n",
    "                \n",
    "        cv2.imshow('Attendance System', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Save attendance data and quit on 'q' press\n",
    "        if key == ord('q'):\n",
    "            wb.save(attendance_file)\n",
    "            break\n",
    "\n",
    "    vs.stop()\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d42ddc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= load_model\n",
    "recognize_and_mark_attendance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf59cdaf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf268a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cead3bbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ccb602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d889d4ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b1d7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (1360, 200, 200, 3)\n",
      "X_val (340, 200, 200, 3)\n",
      "y_train (1360, 17)\n",
      "y_val (340, 17)\n"
     ]
    }
   ],
   "source": [
    "def train_svm_model():\n",
    "\n",
    "    # Split data into training and validation sets (optional)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(images, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "    print('X_train',X_train.shape)\n",
    "    print('X_val',X_val.shape)\n",
    "    print('y_train',y_train.shape)\n",
    "    print('y_val',y_val.shape)\n",
    "\n",
    "    X_train_flat = X_train.reshape(X_train.shape[0], -1)\n",
    "    X_val_flat = X_val.reshape(X_val.shape[0], -1)\n",
    "    \n",
    "      # Create a pipeline with SVM for classification\n",
    "    svm_model = make_pipeline(OneVsRestClassifier(SVC(kernel='linear')))\n",
    "\n",
    "      # Train the SVM model\n",
    "    svm_model.fit(X_train_flat, y_train)\n",
    "\n",
    "    # Evaluate the model on the validation set (optional)\n",
    "    y_preds = svm_model.predict(X_val_flat)\n",
    "    # Convert one-hot encoded labels to categorical labels (if applicable)\n",
    "    y_pred_classes = np.argmax(y_preds, axis=1)\n",
    "    y_val_classes = np.argmax(y_val, axis=1)\n",
    "\n",
    "    # Print classification report and confusion matrix (optional)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_val_classes, y_pred_classes))\n",
    "\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_val_classes, y_pred_classes))\n",
    "\n",
    "    # Save the trained model (optional)\n",
    "    # ... (use your preferred model saving method)\n",
    "    dump(svm_model, 'svm_model.joblib')\n",
    "    print(\"Model training complete!\")\n",
    "    \n",
    "# Train the SVM model\n",
    "train_svm_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a877d2fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Define paths and variables\u001b[39;00m\n\u001b[0;32m      4\u001b[0m label_encoder \u001b[38;5;241m=\u001b[39m LabelEncoder()\n\u001b[1;32m----> 5\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mflatten()  \u001b[38;5;66;03m# Reshape labels to a 1D array\u001b[39;00m\n\u001b[0;32m      6\u001b[0m labels \u001b[38;5;241m=\u001b[39m label_encoder\u001b[38;5;241m.\u001b[39mfit_transform(labels)\n\u001b[0;32m      7\u001b[0m attendance_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattendance.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'labels' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "s_images = []\n",
    "s_labels = []\n",
    "for person_name in os.listdir(data_path):\n",
    "    for filename in os.listdir(os.path.join(data_path, person_name)):\n",
    "        img_path = os.path.join(data_path, person_name, filename)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.resize(img, (200, 200))  # Resize images for CNN input\n",
    "        s_images.append(img)\n",
    "        s_labels.append(person_name)\n",
    "\n",
    "# Convert images and labels to NumPy arrays\n",
    "s_images = np.array(images, dtype=np.float32) / 255.0\n",
    "s_labels = np.array(labels)\n",
    "\n",
    "\n",
    "# One-hot encode labels for categorical crossentropy loss   \n",
    "label_encoder = LabelEncoder()\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "onehot_encoder = OneHotEncoder(sparse_output=False)\n",
    "labels = onehot_encoder.fit_transform(labels.reshape(-1, 1))\n",
    "\n",
    "\n",
    "\n",
    "# Define paths and variables\n",
    "label_encoder = LabelEncoder()\n",
    "labels = labels.flatten()  # Reshape labels to a 1D array\n",
    "labels = label_encoder.fit_transform(labels)\n",
    "attendance_file = \"attendance.xlsx\"\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "recorded_names = set() # Create an empty set to store already recorded names\n",
    "\n",
    "# Function to recognize face and mark attendance\n",
    "def svm_recognize_and_mark_attendance():\n",
    "    \n",
    "    svm_model = load('svm_model.joblib')\n",
    "    global label_encoder  # Access the global label encoder\n",
    "    # Load the label encoder if not already loaded\n",
    "       \n",
    "    # Create an Excel workbook for attendance\n",
    "    wb = Workbook()\n",
    "    ws = wb.active\n",
    "    ws.append([\"Date\", \"Time\", \"Name\"])  # Header row\n",
    "\n",
    "    # Start video stream\n",
    "    vs = VideoStream(src=0).start()\n",
    "    while True:\n",
    "        frame = vs.read()\n",
    "        faces = face_cascade.detectMultiScale(frame, 1.8, 5)\n",
    "        \n",
    "        for (x, y, w, h) in faces:\n",
    "            roi_color = frame[y-68:y+h+18, x-28:x+w+38]\n",
    "            \n",
    "            # Resize for model input\n",
    "            img = cv2.resize(roi_color, (200, 200))\n",
    "            img = np.expand_dims(img, axis=0)  # Add a dimension for batch processing\n",
    "            img = np.array(img, dtype=np.float32) / 255.0\n",
    "            # Flatten the image for SVM model\n",
    "            img_flat = img.reshape(img.shape[0], -1)\n",
    "            \n",
    "            # Predict probability distribution for each class (person)   \n",
    "            svm_predictions=svm_model.predict(img_flat)\n",
    "\n",
    "                # Find the class with the highest probability\n",
    "                \n",
    "            svm_max_index=np.argmax(svm_predictions)   \n",
    "            svm_predicted_name=label_encoder.inverse_transform([svm_max_index])[0]  \n",
    "            svm_proba = svm_predictions[svm_max_index]\n",
    "            print('svm predicted name ',svm_predicted_name)\n",
    "\n",
    "            # Display name and confidence level if probability is high enough\n",
    "            if svm_predicted_name in recorded_names and np.any(svm_proba > 0.8):  # Check if already recorded\n",
    "                print(f\"{svm_predicted_name} already marked attendance.\") \n",
    "                cv2.putText(frame, f\"{svm_predicted_name} ({proba:.2f})\", (x-30, y - 75),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "            elif np.any(svm_proba > 0.8):  # Adjust threshold based on your model's performance\n",
    "                cv2.putText(frame, f\"{svm_predicted_name} ({proba:.2f})\", (x-30, y - 75),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                recorded_names.add(svm_predicted_name)\n",
    "                # Mark attendance in Excel\n",
    "                now = datetime.datetime.now()\n",
    "                ws.append([now.strftime(\"%Y-%m-%d\"), now.strftime(\"%H:%M:%S\"), svm_predicted_name])\n",
    "\n",
    "            else:\n",
    "                cv2.putText(frame, \"Unknown\", (x-30, y - 70),\n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "            cv2.rectangle(frame, (x-30, y-70), (x+w+20, y+h+40), (255, 0, 0), 2)\n",
    "            \n",
    "                \n",
    "        cv2.imshow('Attendance System', frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # Save attendance data and quit on 'q' press\n",
    "        if key == ord('q'):\n",
    "            wb.save(attendance_file)\n",
    "            break\n",
    "\n",
    "    vs.stop()\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    \n",
    "svm_recognize_and_mark_attendance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b68e91a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
